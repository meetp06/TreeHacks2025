{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6050b5a0-82f9-4980-bb6e-6c6b55017d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mglob\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbase64\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import base64\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client with API key\n",
    "client = Groq(\n",
    "   api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06261137-382b-4589-a8e4-22de161326d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n = 10  # Process every nth frame\n",
    "frame_dir = \"frames\"\n",
    "report_path = \"final_report.txt\"  # Path for the report file\n",
    "\n",
    "# Create frames directory if it doesn't exist\n",
    "os.makedirs(frame_dir, exist_ok=True)\n",
    "\n",
    "# Capture from the webcam (use 0 for the default MacBook camera)\n",
    "cap = cv2.VideoCapture(0)\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Take every 10th frame\n",
    "\n",
    "# Open the report file for writing\n",
    "fall_detected = False  # Track if any fall is detected\n",
    "with open(report_path, \"w\") as report_file:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Save every 10th frame\n",
    "        if frame_count % frame_interval == 0:\n",
    "            frame_path = os.path.join(frame_dir, f\"frame_{frame_count:04d}.jpg\")\n",
    "            cv2.imwrite(frame_path, frame)\n",
    "            print(f\"Extracted frame: {frame_path}\")\n",
    "            \n",
    "            # Process the extracted frame for fall detection\n",
    "            with open(frame_path, \"rb\") as image_file:\n",
    "                image_data = image_file.read()\n",
    "\n",
    "            # Encode the image data in base64\n",
    "            image_base64 = base64.b64encode(image_data).decode('utf-8')\n",
    "\n",
    "            # Send a concise query about fall detection\n",
    "            response = client.chat.completions.create(\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an AI model trained to detect falls in images. Your task is to analyze the given image and determine whether a person is falling. A fall is defined as a sudden, unintended, and uncontrolled movement to the ground or a lower level, where the person appears to have lost balance, is collapsing, or is lying in an unnatural posture. If the image shows a person in a falling state or already collapsed, respond with 'Fall Detected.' If the person is standing, walking, sitting, or engaged in any normal activity, respond with 'No Fall Detected.' Your response must be strictly one of the following options: 'Fall Detected.' or 'No Fall Detected.'\"\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": f\"Analyze this frame: {frame_path}. Does it depict a fall?\"\n",
    "                    }\n",
    "                ],\n",
    "                model=\"llama-3.2-11b-vision-preview\"\n",
    "            )\n",
    "\n",
    "            # Extract prediction from the response\n",
    "            prediction = response.choices[0].message.content.strip()\n",
    "            \n",
    "            # Write the result to the report file\n",
    "            report_file.write(f\"Prediction for {frame_path}: {prediction}\\n\")\n",
    "            print(f\"Prediction for {frame_path}: {prediction}\")\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(f\"Final report generated: {report_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2d1a3-f175-4d5f-8048-ebb07ea575cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
